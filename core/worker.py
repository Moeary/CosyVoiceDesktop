import sys
import os
import torch
import random
import torchaudio
from typing import List, Optional
from PyQt5.QtCore import QThread, pyqtSignal

from .models import TaskSegment

class AudioGenerationWorker(QThread):
    """éŸ³é¢‘ç”Ÿæˆå·¥ä½œçº¿ç¨‹"""
    progress = pyqtSignal(str)  # æ—¥å¿—æ¶ˆæ¯
    finished = pyqtSignal(list)  # ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
    error = pyqtSignal(str)  # é”™è¯¯æ¶ˆæ¯
    segment_finished = pyqtSignal(int, list)  # æ®µè½ç´¢å¼•, ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
    
    def __init__(self, segments: List[TaskSegment], output_dir: str, 
                 project_name: str, cosyvoice_model=None):
        super().__init__()
        self.segments = segments
        self.output_dir = output_dir
        self.project_name = project_name
        self.cosyvoice = cosyvoice_model
        self.is_running = True
    
    def stop(self):
        self.is_running = False
    
    def run(self):
        try:
            # å¦‚æœæ²¡æœ‰æ¨¡å‹ï¼Œå…ˆåŠ è½½
            if self.cosyvoice is None:
                self.progress.emit("ğŸ“¦ æ­£åœ¨åŠ è½½CosyVoiceæ¨¡å‹...")
                self.cosyvoice = self.load_model()
                self.progress.emit("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # å¯¼å…¥å¿…è¦çš„æ¨¡å—
            from cosyvoice.utils.file_utils import load_wav
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            # ä¿®æ”¹ï¼šè¾“å‡ºç›®å½•åŒ…å«é¡¹ç›®å
            project_output_dir = os.path.join(self.output_dir, self.project_name)
            os.makedirs(project_output_dir, exist_ok=True)
            
            all_generated_files = []
            
            # æŒ‰æ®µè½ç”Ÿæˆ
            for segment in self.segments:
                if not self.is_running:
                    break
                
                # è®¾ç½®éšæœºç§å­
                torch.manual_seed(segment.seed)
                random.seed(segment.seed)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed(segment.seed)
                    torch.cuda.manual_seed_all(segment.seed)
                
                self.progress.emit(f"ğŸµ æ­£åœ¨ç”Ÿæˆç¬¬ {segment.index} æ®µ...")
                self.progress.emit(f"   æ–‡æœ¬: {segment.text}")
                self.progress.emit(f"   é…ç½®: {segment.voice_config.name} ({segment.mode})")
                self.progress.emit(f"   ç§å­: {segment.seed}")
                
                # åŠ è½½å‚è€ƒéŸ³é¢‘
                if not segment.voice_config.prompt_audio or not os.path.exists(segment.voice_config.prompt_audio):
                    self.progress.emit(f"âš ï¸ å‚è€ƒéŸ³é¢‘ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                    continue
                
                # ä¿®æ”¹ï¼šç›´æ¥ä¼ é€’éŸ³é¢‘è·¯å¾„ï¼Œè€Œä¸æ˜¯åŠ è½½åçš„tensor
                # CosyVoiceå†…éƒ¨ä¼šå¤„ç†éŸ³é¢‘åŠ è½½
                prompt_audio_path = segment.voice_config.prompt_audio
                
                # ç”ŸæˆéŸ³é¢‘ - åŒä¸€æ¬¡è¿è¡Œçš„æ‰€æœ‰ç‰‡æ®µä½œä¸ºä¸€ä¸ªç‰ˆæœ¬
                segment_files = []
                
                inference_func = self.get_inference_function(segment)
                
                for sub_idx, result in enumerate(inference_func(segment, prompt_audio_path)):
                    if not self.is_running:
                        break
                    
                    # ç”Ÿæˆæ–‡ä»¶åï¼šä½¿ç”¨run_count+1ä½œä¸ºç‰ˆæœ¬å·
                    filename = self.generate_filename(segment, sub_idx, segment.run_count + 1)
                    filepath = os.path.join(project_output_dir, filename)
                    
                    # ä¿å­˜éŸ³é¢‘
                    torchaudio.save(filepath, result['tts_speech'], self.cosyvoice.sample_rate)
                    segment_files.append(filepath)
                    all_generated_files.append(filepath)
                    
                    self.progress.emit(f"âœ… ä¿å­˜: {filename}")
                
                # å°†è¿™ä¸€æ‰¹æ–‡ä»¶ä½œä¸ºæ–°ç‰ˆæœ¬æ·»åŠ 
                if segment_files:
                    segment.add_version(segment_files)
                    self.progress.emit(f"ğŸ“¦ ç‰ˆæœ¬ v{segment.run_count} åŒ…å« {len(segment_files)} ä¸ªç‰‡æ®µ")
                
                # å‘é€æ®µè½å®Œæˆä¿¡å·
                self.segment_finished.emit(segment.index, segment_files)
            
            if self.is_running:
                self.finished.emit(all_generated_files)
            
        except Exception as e:
            self.error.emit(f"ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    def load_model(self):
        """åŠ è½½CosyVoiceæ¨¡å‹"""
        # ä¼˜å…ˆå°è¯•CosyVoice3
        model_dir = 'pretrained_models/Fun-CosyVoice3-0.5B'
        if not os.path.exists(model_dir):
            # å°è¯•å¦ä¸€ä¸ªå¯èƒ½çš„ç›®å½•å
            model_dir = 'pretrained_models/CosyVoice3-0.5B'
            
        if not os.path.exists(model_dir):
            # å›é€€åˆ°CosyVoice2
            model_dir = 'pretrained_models/CosyVoice2-0.5B'
            
        if not os.path.exists(model_dir):
            raise FileNotFoundError(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
        
        sys.path.append('third_party/Matcha-TTS')
        from cosyvoice.cli.cosyvoice import AutoModel
        
        # AutoModelä¼šè‡ªåŠ¨æ ¹æ®yamlé€‰æ‹©æ­£ç¡®çš„æ¨¡å‹ç±»
        # æ³¨æ„ï¼šCosyVoice3ä¸æ”¯æŒload_jitå‚æ•°
        return AutoModel(
            model_dir=model_dir, 
            load_trt=False, 
            load_vllm=False, 
            fp16=False
        )
    
    def get_inference_function(self, segment: TaskSegment):
        """è·å–æ¨ç†å‡½æ•°"""
        # æ£€æŸ¥æ˜¯å¦ä¸ºCosyVoice3æ¨¡å‹
        is_v3 = 'CosyVoice3' in getattr(self.cosyvoice, 'model_dir', '')
        
        if segment.mode == 'é›¶æ ·æœ¬å¤åˆ¶':
            def inference(seg, prompt_audio):
                prompt_text = seg.voice_config.prompt_text
                # CosyVoice3éœ€è¦ç‰¹å®šçš„promptæ ¼å¼
                if is_v3 and '<|endofprompt|>' not in prompt_text:
                    prompt_text = f'You are a helpful assistant.<|endofprompt|>{prompt_text}'
                
                return self.cosyvoice.inference_zero_shot(
                    seg.text, prompt_text, 
                    prompt_audio, stream=False
                )
            return inference
        
        elif segment.mode == 'ç²¾ç»†æ§åˆ¶':
            def inference(seg, prompt_audio):
                text = seg.text
                # CosyVoice3ç²¾ç»†æ§åˆ¶éœ€è¦åœ¨æ–‡æœ¬å‰åŠ æŒ‡ä»¤
                if is_v3 and '<|endofprompt|>' not in text:
                    text = f'You are a helpful assistant.<|endofprompt|>{text}'
                
                return self.cosyvoice.inference_cross_lingual(
                    text, prompt_audio, stream=False
                )
            return inference
        
        elif segment.mode == 'æŒ‡ä»¤æ§åˆ¶':
            def inference(seg, prompt_audio):
                instruct_text = seg.instruct_text
                # CosyVoice3æŒ‡ä»¤éœ€è¦ä»¥<|endofprompt|>ç»“å°¾ï¼Œä¸”é€šå¸¸éœ€è¦"You are a helpful assistant."å‰ç¼€
                if is_v3:
                    # ç¡®ä¿æŒ‡ä»¤åœ¨ä¸­é—´ï¼šYou are a helpful assistant. {instruct_text}<|endofprompt|>
                    if '<|endofprompt|>' not in instruct_text:
                        instruct_text = f'{instruct_text}<|endofprompt|>'
                    if 'You are a helpful assistant.' not in instruct_text:
                        instruct_text = f'You are a helpful assistant. {instruct_text}'
                
                # ä½¿ç”¨ inference_instruct2
                return self.cosyvoice.inference_instruct2(
                    seg.text, instruct_text, 
                    prompt_audio, stream=False
                )
            return inference
        
        elif segment.mode == 'è¯­éŸ³ä¿®è¡¥':
            def inference(seg, prompt_audio):
                prompt_text = seg.voice_config.prompt_text
                # è¯­éŸ³ä¿®è¡¥æœ¬è´¨ä¸Šæ˜¯Zero-Shotï¼Œä½†æ–‡æœ¬å¯èƒ½åŒ…å«å‘éŸ³ä¿®æ­£æ ‡è®°
                if is_v3 and '<|endofprompt|>' not in prompt_text:
                    prompt_text = f'You are a helpful assistant.<|endofprompt|>{prompt_text}'
                
                return self.cosyvoice.inference_zero_shot(
                    seg.text, prompt_text, 
                    prompt_audio, stream=False
                )
            return inference
        
        else:  # é»˜è®¤å›é€€åˆ°é›¶æ ·æœ¬
            def inference(seg, prompt_audio):
                prompt_text = seg.voice_config.prompt_text
                if is_v3 and '<|endofprompt|>' not in prompt_text:
                    prompt_text = f'You are a helpful assistant.<|endofprompt|>{prompt_text}'
                
                return self.cosyvoice.inference_zero_shot(
                    seg.text, prompt_text, 
                    prompt_audio, stream=False
                )
            return inference
    
    def generate_filename(self, segment: TaskSegment, sub_index: int, version: int) -> str:
        """ç”Ÿæˆæ–‡ä»¶å: æ®µè½åºå·_ç‰ˆæœ¬å·_æ–‡æœ¬é¢„è§ˆ_ç‰‡æ®µåºå·.wav"""
        # æ–‡æœ¬é¢„è§ˆï¼ˆ10ä¸ªå­—ç¬¦ï¼‰
        text_preview = self.sanitize_filename(segment.text[:10])
        
        # æ ¼å¼ï¼šæ®µè½_ç‰ˆæœ¬_æ–‡æœ¬_ç‰‡æ®µ.wav
        # åªæœ‰ä¸€ä¸ªç‰‡æ®µæ—¶ä¸æ˜¾ç¤ºç‰‡æ®µå·
        return f"{segment.index}_{version}_{text_preview}_{sub_index+1}.wav"
    
    def sanitize_filename(self, text: str) -> str:
        """å¤„ç†æ–‡ä»¶åï¼Œç¬¦åˆWindowsè§„åˆ™"""
        invalid_chars = '<>:"/\\|?*'
        for char in invalid_chars:
            text = text.replace(char, '')
        text = ''.join(char for char in text if ord(char) >= 32)
        text = text.replace(' ', '_').replace('\n', '_').replace('\t', '_')
        while '__' in text:
            text = text.replace('__', '_')
        text = text.strip('_')
        return text or 'audio'
